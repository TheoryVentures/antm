{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Start Example (Optional)\n",
        "\n",
        "This notebook demonstrates basic data access patterns using DuckDB and pandas. For a deeper dive into building full analytics agents on MotherDuck (prompt design, MCP integration, security), see [MotherDuck’s analytics agent guide](https://motherduck.com/docs/key-tasks/ai-and-motherduck/building-analytics-agents/).\n",
        "\n",
        "## Contents\n",
        "- Tool installation\n",
        "- Loading Parquet files\n",
        "- SQL queries\n",
        "- Log file parsing\n",
        "- Submission format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Tools\n",
        "\n",
        "Run this once:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -q duckdb pandas pyarrow lancedb openai\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to DuckDB\n",
        "\n",
        "DuckDB lets you run SQL queries on Parquet files. It's fast and works well for analytics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "con = duckdb.connect('retail.duckdb')\n",
        "print(\"Connected to DuckDB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Load Data into local DuckDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Parquet files into DuckDB so you can query them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all parquet files in our GCS bucket\n",
        "files = con.execute(\"SELECT file FROM glob('gs://antm-dataset/**/*.parquet LIMIT 10')\").fetchall()\n",
        "\n",
        "# Create table for each file\n",
        "for file_path, in files:\n",
        "    table_name = Path(file_path).stem\n",
        "    con.execute(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM read_parquet('{file_path}')\")\n",
        "\n",
        "# Show tables\n",
        "print(f\"\\nCreated {len(files)} tables\")\n",
        "con.execute(\"SHOW TABLES\").fetchmany()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 2: Connect to MotherDuck\n",
        "\n",
        "MotherDuck gives your local DuckDB cloud compute resources. It also lets you share data with others easily. \n",
        "\n",
        "1. Go to [app.motherduck.com](https://app.motherduck.com) and create an account.\n",
        "2. [create an access token](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/authenticating-to-motherduck/#creating-an-access-token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"motherduck_token\"] = \"your_actual_token_here\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create [read-write clone](https://motherduck.com/docs/key-tasks/ai-and-motherduck/building-analytics-agents/#read-write-access--sandboxing) of 'antm_hack' MotherDuck Share."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.environ.get(\"motherduck_token\") == \"your_actual_token_here\":\n",
        "    print(\"Using local DuckDB\")\n",
        "else:\n",
        "    print(\"Creating 'antm_hack_rw' database, from 'antm_hack' share\")\n",
        "\n",
        "    con.execute(\"CREATE DATABASE antm_hack_rw FROM 'md:_share/antm_hack/88329567-1b97-4593-9696-73fd2be9c63d'\")\n",
        "    con.execute(\"USE antm_hack_rw\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run a Query\n",
        "\n",
        "Example: Find top customers by spending\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if customer_path.exists():\n",
        "    query = \"\"\"\n",
        "    SELECT \n",
        "        c_customer_sk,\n",
        "        c_first_name,\n",
        "        c_last_name\n",
        "    FROM customer\n",
        "    ORDER BY c_customer_sk\n",
        "    LIMIT 5\n",
        "    \"\"\"\n",
        "    \n",
        "    result = con.execute(query).df()\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Log Files\n",
        "\n",
        "Logs are in JSONL format (one JSON object per line). You can use pandas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_path = Path('../dataset/data/logs/customer_service.jsonl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ex_path.exists():\n",
        "    logs = pd.read_json('../dataset/data/logs/customer_service.jsonl', lines=True)\n",
        "    print(f\"Loaded {len(logs)} events\")\n",
        "    print(logs.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Query Logs with DuckDB\n",
        "\n",
        "You can also load logs into DuckDB and use SQL:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ex_path.exists():\n",
        "    con.execute(\"CREATE OR REPLACE TABLE logs AS SELECT * FROM read_json_auto('../dataset/data/logs/customer_service.jsonl')\")\n",
        "    \n",
        "    result = con.execute(\"\"\"\n",
        "        SELECT category, COUNT(*) as count \n",
        "        FROM logs \n",
        "        GROUP BY category\n",
        "        ORDER BY count DESC\n",
        "        LIMIT 10\n",
        "    \"\"\").df()\n",
        "    \n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using LanceDB for Semantic Search\n",
        "\n",
        "LanceDB is useful when you need to search by meaning rather than exact matches. Example use case: searching through log descriptions or PDF content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"your_actual_openai_api_key_here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lancedb\n",
        "import pydantic\n",
        "from openai import OpenAI\n",
        "\n",
        "# Connect to LanceDB\n",
        "db = lancedb.connect('lance_db')\n",
        "\n",
        "# Create openai client for embedding generation\n",
        "client = OpenAI()\n",
        "\n",
        "# Example: Load log data into LanceDB\n",
        "if ex_path.exists():\n",
        "    log_data = pd.read_json(ex_path, lines=True)\n",
        "    with open(ex_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        json_strings = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    # split to_embed into batches of 1000, generate embeddings, collect results in a list\n",
        "    batch_size = 1000\n",
        "    embeddings = []\n",
        "    for i in range(0, len(json_strings), batch_size):\n",
        "        batch = json_strings[i:i+batch_size]\n",
        "        resp = client.embeddings.create(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            input=batch,\n",
        "        )\n",
        "        batch_embeds = [item.embedding for item in resp.data]\n",
        "        embeddings.extend(batch_embeds)\n",
        "        print(f\"Embedded batch {(i // batch_size) + 1}\")\n",
        "    log_data['vector'] = embeddings\n",
        "\n",
        "    # Create a table\n",
        "    table = db.create_table(\"logs\", data=log_data, mode=\"overwrite\")\n",
        "    \n",
        "    print(f\"Loaded {len(log_data)} rows into LanceDB\")\n",
        "else:\n",
        "    print(\"Log files not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Run semantic search by embedding a query and searching the vector store\n",
        "query = \"which items don't we have in inventory?\"\n",
        "\n",
        "# generate query embedding using OpenAI\n",
        "query_embedding = client.embeddings.create(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    input=[query],\n",
        ").data[0].embedding\n",
        "\n",
        "results = table.search(query_embedding).limit(10).to_pandas()\n",
        "print(\"\\nSemantic search results:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Query LanceDB table or pandas DataFrame with DuckDB\n",
        "arrow_table = table.to_lance()\n",
        "con.query(\"SELECT * FROM arrow_table\")\n",
        "con.query(\"SELECT * FROM results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission Format\n",
        "\n",
        "Save your answers in CSV format:\n",
        "\n",
        "```csv\n",
        "question_id,answer_type,answer_value,confidence,explanation\n",
        "1,customer_id,12345,high,Top customer by revenue\n",
        "1,total_spent,50000,high,Sum of net_paid\n",
        "```\n",
        "\n",
        "Create submissions programmatically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.DataFrame([\n",
        "    {'question_id': 1, 'answer_type': 'customer_id', 'answer_value': 12345, 'confidence': 'high', 'explanation': 'Top customer by revenue'},\n",
        "    {'question_id': 1, 'answer_type': 'total_spent', 'answer_value': 50000, 'confidence': 'high', 'explanation': 'Sum of net_paid'},\n",
        "])\n",
        "\n",
        "print(submission)\n",
        "\n",
        "# To save:\n",
        "# submission.to_csv('my_submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Competition Structure\n",
        "\n",
        "**Training Round (12:30-2:00):** 25 questions with answers provided. Practice only, no submission required.\n",
        "\n",
        "**Test Round (2:00-6:00):** 30 questions without answers. Submit by 6:00 PM. Worth 70% of final score.\n",
        "\n",
        "**Holdout Round (6:00-7:30):** 20 secret questions. We run your system automatically. Worth 30% of final score.\n",
        "\n",
        "That's the basics. Check the README for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool Use Cases\n",
        "\n",
        "**DuckDB:** SQL queries on structured data (Parquet tables) and logs. Fast for aggregations, joins, filtering.\n",
        "\n",
        "**LanceDB:** Semantic search when you need to find things by meaning, not exact matches. Good for searching PDFs or finding similar log entries.\n",
        "\n",
        "**MotherDuck:** Cloud version of DuckDB. Useful for sharing data/queries with teammates or working with larger datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCP Server Quickstart\n",
        "\n",
        "1. **Pick a language/runtime.** MCP servers only need stdin/stdout plus JSON-RPC. Python (`mcp`), TypeScript (`@modelcontextprotocol/server`), or Go all work.\n",
        "2. **Choose resources.** Decide what data you’ll expose (DuckDB tables, Parquet files, PDF search). Give each a stable URI so clients know how to reference them.\n",
        "3. **Implement tools.** Each MCP tool wraps an action—run SQL, summarize a log window, fetch a PDF section. Keep inputs/outputs typed and minimal so LLMs can call them safely.\n",
        "4. **Advertise capabilities.** In `initialize` return your tool/resource metadata so Cursor/Claude Desktop lists them automatically.\n",
        "5. **Run & register.** Start the server (e.g., `python mcp_server.py`) and add that command under `Cursor → Settings → MCP Servers`.\n",
        "\n",
        "### DuckDB FastMCP example\n",
        "\n",
        "Here's a minimal Python scaffolding for DuckDB:\n",
        "\n",
        "```python\n",
        "from fastmcp import FastMCP\n",
        "import duckdb\n",
        "\n",
        "con = duckdb.connect('retail.duckdb')\n",
        "mcp = FastMCP()\n",
        "\n",
        "@mcp.tool(name=\"duckdb_describe_customer\")\n",
        "def describe_customer(limit: int = 5) -> list[dict[str, str]]:\n",
        "    \"\"\"Return sample customer rows from DuckDB.\"\"\"\n",
        "    rows = con.execute(\n",
        "        \"SELECT c_customer_sk, c_first_name, c_last_name FROM customer LIMIT ?\",\n",
        "        [limit],\n",
        "    ).fetchall()\n",
        "    return [dict(row) for row in rows]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run()\n",
        "```\n",
        "\n",
        "Register `python mcp_server.py` as a custom MCP server and the `describe_customer` tool becomes available directly in your prompts.\n",
        "\n",
        "> **Shortcut:** Don’t want to build your own? MotherDuck ships an OSS MCP server that connects to both DuckDB and MotherDuck backends, complete with SaaS/read-only modes and Claude/Cursor examples. Install it via `uvx mcp-server-motherduck …` using the instructions in their repo: https://github.com/motherduckdb/mcp-server-motherduck.\n",
        "\n",
        "### LanceDB FastMCP example\n",
        "\n",
        "LanceDB also provides an MCP server that exposes read and write tools. Here's a minimal example.\n",
        "\n",
        "```python\n",
        "from mcp.server.fastmcp import FastMCPServer\n",
        "import lancedb\n",
        "\n",
        "# Re-use the `lance_db` directory populated earlier\n",
        "# (or point to any LanceDB-backed dataset you want to expose)\n",
        "db = lancedb.connect(\"lance_db\")\n",
        "server = FastMCPServer()\n",
        "\n",
        "@mcp.tool(name=\"lancedb_search_logs\")\n",
        "def search_logs(query: str, limit: int = 5) -> list[dict]:\n",
        "    \"\"\"Return log rows semantically similar to the text query.\"\"\"\n",
        "    table = db.open_table(\"logs\")\n",
        "    matches = table.search(query).limit(limit).to_list()\n",
        "    return matches\n",
        "\n",
        "@server.tool(name=\"lancedb_list_tables\")\n",
        "def list_tables() -> list[str]:\n",
        "    \"\"\"List available LanceDB tables.\"\"\"\n",
        "    return db.table_names()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    server.run()\n",
        "```\n",
        "\n",
        "Register this script the same way as the DuckDB example and you now have semantic search tools on tap. Prefer a maintained implementation instead? The LanceDB team ships a ready-to-run FastMCP server (CLI plus config examples) here: https://github.com/lancedb/lancedb-mcp-server.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "antm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
